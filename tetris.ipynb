{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.6.1 (SDL 2.28.4, Python 3.12.8)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from collections import deque, namedtuple\n",
    "import pygame\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SHAPES = {\n",
    "    'I': [[(0,0), (0,1), (0,2), (0,3)],\n",
    "          [(0,0), (1,0), (2,0), (3,0)]],\n",
    "    'O': [[(0,0), (0,1), (1,0), (1,1)]],\n",
    "    'T': [[(0,1), (1,0), (1,1), (1,2)],\n",
    "          [(0,1), (1,1), (1,2), (2,1)],\n",
    "          [(1,0), (1,1), (1,2), (2,1)],\n",
    "          [(0,1), (1,0), (1,1), (2,1)]],\n",
    "    'L': [[(0,2), (1,0), (1,1), (1,2)],\n",
    "          [(0,1), (1,1), (2,1), (2,2)],\n",
    "          [(1,0), (1,1), (1,2), (2,0)],\n",
    "          [(0,0), (0,1), (1,1), (2,1)]],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TetrisEnv:\n",
    "    def __init__(self, width=10, height=20):\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.board = None\n",
    "        self.current_piece = None\n",
    "        self.current_pos = None\n",
    "        self.score = 0\n",
    "        \n",
    "        # Add color mapping for pieces\n",
    "        self.colors = {\n",
    "            'I': (0, 255, 255),    # Cyan\n",
    "            'O': (255, 255, 0),    # Yellow\n",
    "            'T': (128, 0, 128),    # Purple\n",
    "            'L': (255, 165, 0),    # Orange\n",
    "            'grid': (128, 128, 128) # Gray\n",
    "        }\n",
    "        \n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.board = np.zeros((self.height, self.width), dtype=int)\n",
    "        self.spawn_piece()\n",
    "        return self._get_state()\n",
    "    \n",
    "    def spawn_piece(self):\n",
    "        self.current_piece = random.choice(list(SHAPES.keys()))\n",
    "        self.current_rotation = 0\n",
    "        self.current_pos = [0, self.width // 2 - 1]\n",
    "    \n",
    "    def _get_current_shape(self):\n",
    "        return SHAPES[self.current_piece][self.current_rotation]\n",
    "    \n",
    "    def _is_valid_move(self, pos, shape):\n",
    "        for block in shape:\n",
    "            y = pos[0] + block[0]\n",
    "            x = pos[1] + block[1]\n",
    "            if (x < 0 or x >= self.width or \n",
    "                y >= self.height or \n",
    "                (y >= 0 and self.board[y][x])):\n",
    "                return False\n",
    "        return True\n",
    "    \n",
    "    def _clear_lines(self):\n",
    "        lines_cleared = 0\n",
    "        y = self.height - 1\n",
    "        while y >= 0:\n",
    "            if np.all(self.board[y]):\n",
    "                self.board = np.vstack((np.zeros((1, self.width)), \n",
    "                                      self.board[:y], \n",
    "                                      self.board[y+1:]))\n",
    "                lines_cleared += 1\n",
    "            else:\n",
    "                y -= 1\n",
    "        return lines_cleared\n",
    "    \n",
    "    def _get_state(self):\n",
    "        # Returns the current state as a flat array\n",
    "        return np.append(self.board.flatten(), \n",
    "                        [self.current_pos[0], self.current_pos[1], \n",
    "                         self.current_rotation])\n",
    "    \n",
    "    def step(self, action):\n",
    "        # Actions: 0: left, 1: right, 2: rotate, 3: down\n",
    "        reward = 0\n",
    "        done = False\n",
    "        \n",
    "        if action == 0:  # Move left\n",
    "            new_pos = [self.current_pos[0], self.current_pos[1] - 1]\n",
    "            if self._is_valid_move(new_pos, self._get_current_shape()):\n",
    "                self.current_pos = new_pos\n",
    "                \n",
    "        elif action == 1:  # Move right\n",
    "            new_pos = [self.current_pos[0], self.current_pos[1] + 1]\n",
    "            if self._is_valid_move(new_pos, self._get_current_shape()):\n",
    "                self.current_pos = new_pos\n",
    "                \n",
    "        elif action == 2:  # Rotate\n",
    "            new_rotation = (self.current_rotation + 1) % len(SHAPES[self.current_piece])\n",
    "            new_shape = SHAPES[self.current_piece][new_rotation]\n",
    "            if self._is_valid_move(self.current_pos, new_shape):\n",
    "                self.current_rotation = new_rotation\n",
    "                \n",
    "        # Always try to move down\n",
    "        new_pos = [self.current_pos[0] + 1, self.current_pos[1]]\n",
    "        if self._is_valid_move(new_pos, self._get_current_shape()):\n",
    "            self.current_pos = new_pos\n",
    "        else:\n",
    "            # Place the piece\n",
    "            for block in self._get_current_shape():\n",
    "                y = self.current_pos[0] + block[0]\n",
    "                x = self.current_pos[1] + block[1]\n",
    "                if y >= 0:\n",
    "                    self.board[y][x] = 1\n",
    "            \n",
    "            # Check for cleared lines\n",
    "            lines_cleared = self._clear_lines()\n",
    "            reward = lines_cleared ** 2\n",
    "            \n",
    "            # Spawn new piece\n",
    "            self.spawn_piece()\n",
    "            if not self._is_valid_move(self.current_pos, self._get_current_shape()):\n",
    "                done = True\n",
    "                reward = -10\n",
    "        \n",
    "        return self._get_state(), reward, done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TetrisVisualizer:\n",
    "    def __init__(self, env, cell_size=30):\n",
    "        pygame.init()\n",
    "        self.env = env\n",
    "        self.cell_size = cell_size\n",
    "        self.width = env.width * cell_size\n",
    "        self.height = env.height * cell_size\n",
    "        self.screen = pygame.display.set_mode((self.width, self.height))\n",
    "        pygame.display.set_caption('Tetris AI')\n",
    "\n",
    "        # Define default color for placed blocks\n",
    "        self.placed_block_color = (0, 255, 255)  # Cyan\n",
    "        \n",
    "    def draw_board(self):\n",
    "        self.screen.fill((0, 0, 0))\n",
    "        \n",
    "        # Draw the grid\n",
    "        for y in range(self.env.height):\n",
    "            for x in range(self.env.width):\n",
    "                pygame.draw.rect(self.screen, self.env.colors['grid'],\n",
    "                               [x * self.cell_size, y * self.cell_size,\n",
    "                                self.cell_size, self.cell_size], 1)\n",
    "        \n",
    "        # Draw the placed pieces\n",
    "        for y in range(self.env.height):\n",
    "            for x in range(self.env.width):\n",
    "                if self.env.board[y][x]:\n",
    "                    pygame.draw.rect(self.screen, self.env.colors['I'],\n",
    "                                   [x * self.cell_size, y * self.cell_size,\n",
    "                                    self.cell_size, self.cell_size])\n",
    "        \n",
    "        # Draw the current piece\n",
    "        if self.env.current_piece:\n",
    "            for block in self.env._get_current_shape():\n",
    "                x = (self.env.current_pos[1] + block[1]) * self.cell_size\n",
    "                y = (self.env.current_pos[0] + block[0]) * self.cell_size\n",
    "                if self.env.current_pos[0] + block[0] >= 0:\n",
    "                    pygame.draw.rect(self.screen, \n",
    "                                   self.env.colors[self.env.current_piece],\n",
    "                                   [x, y, self.cell_size, self.cell_size])\n",
    "        \n",
    "        pygame.display.flip()\n",
    "    \n",
    "    def close(self):\n",
    "        pygame.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "        self.transition = namedtuple('Transition',\n",
    "                                   ('state', 'action', 'next_state', 'reward'))\n",
    "    \n",
    "    def push(self, *args):\n",
    "        self.memory.append(self.transition(*args))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TetrisAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.policy_net = DQN(state_size, action_size).to(self.device)\n",
    "        self.target_net = DQN(state_size, action_size).to(self.device)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters())\n",
    "        self.memory = ReplayMemory(10000)\n",
    "        \n",
    "        self.batch_size = 128\n",
    "        self.gamma = 0.99\n",
    "        self.eps_start = 0.9\n",
    "        self.eps_end = 0.05\n",
    "        self.eps_decay = 200\n",
    "        self.target_update = 10\n",
    "        self.steps_done = 0\n",
    "        \n",
    "    def select_action(self, state):\n",
    "        sample = random.random()\n",
    "        eps_threshold = self.eps_end + (self.eps_start - self.eps_end) * \\\n",
    "                       np.exp(-1. * self.steps_done / self.eps_decay)\n",
    "        self.steps_done += 1\n",
    "        \n",
    "        if sample > eps_threshold:\n",
    "            with torch.no_grad():\n",
    "                state = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "                return self.policy_net(state).max(1)[1].view(1, 1)\n",
    "        else:\n",
    "            return torch.tensor([[random.randrange(4)]], \n",
    "                              device=self.device, dtype=torch.long)\n",
    "    \n",
    "    def optimize_model(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        transitions = self.memory.sample(self.batch_size)\n",
    "        batch = self.memory.transition(*zip(*transitions))\n",
    "        \n",
    "        non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                              batch.next_state)), \n",
    "                                    device=self.device, dtype=torch.bool)\n",
    "        non_final_next_states = torch.FloatTensor([s for s in batch.next_state\n",
    "                                                 if s is not None]).to(self.device)\n",
    "        \n",
    "        state_batch = torch.FloatTensor(batch.state).to(self.device)\n",
    "        action_batch = torch.cat(batch.action)\n",
    "        reward_batch = torch.FloatTensor(batch.reward).to(self.device)\n",
    "        \n",
    "        state_action_values = self.policy_net(state_batch).gather(1, action_batch)\n",
    "        \n",
    "        next_state_values = torch.zeros(self.batch_size, device=self.device)\n",
    "        next_state_values[non_final_mask] = self.target_net(non_final_next_states).max(1)[0].detach()\n",
    "        expected_state_action_values = (next_state_values * self.gamma) + reward_batch\n",
    "        \n",
    "        criterion = nn.SmoothL1Loss()\n",
    "        loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        for param in self.policy_net.parameters():\n",
    "            param.grad.data.clamp_(-1, 1)\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function with visualization\n",
    "def train_with_visualization(num_episodes=1000, visualize_every=100):\n",
    "    env = TetrisEnv()\n",
    "    visualizer = TetrisVisualizer(env)\n",
    "    state_size = env.width * env.height + 3\n",
    "    action_size = 4\n",
    "    \n",
    "    agent = TetrisAgent(state_size, action_size)\n",
    "    scores = []\n",
    "    \n",
    "    try:\n",
    "        for episode in range(num_episodes):\n",
    "            state = env.reset()\n",
    "            total_reward = 0\n",
    "            \n",
    "            # Visualize every nth episode\n",
    "            should_visualize = episode % visualize_every == 0\n",
    "            \n",
    "            while True:\n",
    "                # Handle pygame events\n",
    "                for event in pygame.event.get():\n",
    "                    if event.type == pygame.QUIT:\n",
    "                        visualizer.close()\n",
    "                        return agent, scores\n",
    "                \n",
    "                action = agent.select_action(state)\n",
    "                next_state, reward, done = env.step(action.item())\n",
    "                total_reward += reward\n",
    "                \n",
    "                if should_visualize:\n",
    "                    visualizer.draw_board()\n",
    "                    time.sleep(0.05)\n",
    "                \n",
    "                if done:\n",
    "                    next_state = None\n",
    "                \n",
    "                agent.memory.push(state, action, next_state, reward)\n",
    "                state = next_state\n",
    "                \n",
    "                agent.optimize_model()\n",
    "                \n",
    "                if done:\n",
    "                    break\n",
    "            \n",
    "            if episode % agent.target_update == 0:\n",
    "                agent.target_net.load_state_dict(agent.policy_net.state_dict())\n",
    "            \n",
    "            scores.append(total_reward)\n",
    "            if episode % 10 == 0:\n",
    "                print(f\"Episode {episode}, Score: {total_reward}, Average Score: {np.mean(scores[-100:])}\")\n",
    "    \n",
    "    finally:\n",
    "        visualizer.close()\n",
    "    \n",
    "    return agent, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save and load functions for the trained model\n",
    "def save_model(agent, filename):\n",
    "    torch.save({\n",
    "        'policy_net_state_dict': agent.policy_net.state_dict(),\n",
    "        'target_net_state_dict': agent.target_net.state_dict(),\n",
    "        'optimizer_state_dict': agent.optimizer.state_dict(),\n",
    "    }, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(filename, state_size, action_size):\n",
    "    agent = TetrisAgent(state_size, action_size)\n",
    "    checkpoint = torch.load(filename)\n",
    "    agent.policy_net.load_state_dict(checkpoint['policy_net_state_dict'])\n",
    "    agent.target_net.load_state_dict(checkpoint['target_net_state_dict'])\n",
    "    agent.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    return agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_ai_play(agent, env, visualizer, num_games=5, delay=0.1):\n",
    "    for game in range(num_games):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            # Handle pygame events\n",
    "            for event in pygame.event.get():\n",
    "                if event.type == pygame.QUIT:\n",
    "                    visualizer.close()\n",
    "                    return\n",
    "            \n",
    "            # Get AI action\n",
    "            with torch.no_grad():\n",
    "                state_tensor = torch.FloatTensor(state).unsqueeze(0).to(agent.device)\n",
    "                action = agent.policy_net(state_tensor).max(1)[1].view(1, 1)\n",
    "            \n",
    "            # Take step in environment\n",
    "            state, reward, done = env.step(action.item())\n",
    "            total_reward += reward\n",
    "            \n",
    "            # Update visualization\n",
    "            visualizer.draw_board()\n",
    "            time.sleep(delay)  # Add delay to make visualization visible\n",
    "            \n",
    "        print(f\"Game {game + 1} Score: {total_reward}\")\n",
    "    \n",
    "    visualizer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TetrisEnv' object has no attribute 'colors'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Train for 1000 episodes\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m agent, scores \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_with_visualization\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Save the trained model\u001b[39;00m\n\u001b[0;32m      5\u001b[0m save_model(agent, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtetris_model.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[22], line 31\u001b[0m, in \u001b[0;36mtrain_with_visualization\u001b[1;34m(num_episodes, visualize_every)\u001b[0m\n\u001b[0;32m     28\u001b[0m total_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_visualize:\n\u001b[1;32m---> 31\u001b[0m     \u001b[43mvisualizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw_board\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.05\u001b[39m)\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m done:\n",
      "Cell \u001b[1;32mIn[20], line 17\u001b[0m, in \u001b[0;36mTetrisVisualizer.draw_board\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m y \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mheight):\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mwidth):\n\u001b[1;32m---> 17\u001b[0m         pygame\u001b[38;5;241m.\u001b[39mdraw\u001b[38;5;241m.\u001b[39mrect(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscreen, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolors\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgrid\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     18\u001b[0m                        [x \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcell_size, y \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcell_size,\n\u001b[0;32m     19\u001b[0m                         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcell_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcell_size], \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Draw the placed pieces\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m y \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mheight):\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'TetrisEnv' object has no attribute 'colors'"
     ]
    }
   ],
   "source": [
    "# Train for 1000 episodes\n",
    "agent, scores = train_with_visualization(num_episodes=1000, visualize_every=100)\n",
    "\n",
    "# Save the trained model\n",
    "save_model(agent, 'tetris_model.pth')\n",
    "\n",
    "# Load a trained model later\n",
    "state_size = 203  # 10x20 board + 3 for position and rotation\n",
    "action_size = 4\n",
    "loaded_agent = load_model('tetris_model.pth', state_size, action_size)\n",
    "\n",
    "# Load a trained model\n",
    "state_size = 203  # 10x20 board + 3 for position and rotation\n",
    "action_size = 4\n",
    "agent = load_model('tetris_model.pth', state_size, action_size)\n",
    "\n",
    "# Create environment and visualizer\n",
    "env = TetrisEnv()\n",
    "visualizer = TetrisVisualizer(env)\n",
    "\n",
    "# Watch the AI play 5 games with 0.1 second delay between moves\n",
    "visualize_ai_play(agent, env, visualizer, num_games=5, delay=0.1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
