{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.6.1 (SDL 2.28.4, Python 3.12.8)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from collections import deque, namedtuple\n",
    "import pygame\n",
    "import copy\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# For reproducibility\n",
    "random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SHAPES = {\n",
    "    'I': [[(0,0), (0,1), (0,2), (0,3)],\n",
    "          [(0,0), (1,0), (2,0), (3,0)]],\n",
    "    'O': [[(0,0), (0,1), (1,0), (1,1)]],\n",
    "    'T': [[(0,1), (1,0), (1,1), (1,2)],\n",
    "          [(0,1), (1,1), (1,2), (2,1)],\n",
    "          [(1,0), (1,1), (1,2), (2,1)],\n",
    "          [(0,1), (1,0), (1,1), (2,1)]],\n",
    "    'L': [[(0,2), (1,0), (1,1), (1,2)],\n",
    "          [(0,1), (1,1), (2,1), (2,2)],\n",
    "          [(1,0), (1,1), (1,2), (2,0)],\n",
    "          [(0,0), (0,1), (1,1), (2,1)]],\n",
    "}\n",
    "\n",
    "# Define Transition at module level\n",
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TetrisEnv:\n",
    "    def __init__(self, width=10, height=20):\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.board = None\n",
    "        self.current_piece = None\n",
    "        self.current_pos = None\n",
    "        self.current_rotation = None\n",
    "        self.score = 0\n",
    "        \n",
    "        # Enhanced color scheme\n",
    "        self.colors = {\n",
    "            'I': (0, 240, 240),     # Bright Cyan\n",
    "            'O': (240, 240, 0),     # Bright Yellow\n",
    "            'T': (160, 0, 240),     # Bright Purple\n",
    "            'L': (240, 160, 0),     # Bright Orange\n",
    "            'grid': (40, 40, 40),   # Darker Gray\n",
    "            'background': (0, 0, 0), # Black\n",
    "            'ghost': (128, 128, 128, 128)  # Semi-transparent ghost piece\n",
    "        }\n",
    "        \n",
    "        # Reward weights\n",
    "        self.reward_weights = {\n",
    "            'lines_cleared': 20,      # Base multiplier for lines cleared\n",
    "            'tetris_bonus': 50,       # Additional bonus for clearing 4 lines\n",
    "            'height_penalty': 2.5,    # Penalty for stack height\n",
    "            'hole_penalty': 3.0,      # Penalty for creating holes\n",
    "            'surface_penalty': 0.2,   # Penalty for surface roughness\n",
    "            'well_reward': 0.3,       # Reward for creating wells for long pieces\n",
    "            'clear_field_bonus': 100, # Bonus for clearing the field\n",
    "            'game_over_penalty': 50   # Penalty for game over\n",
    "        }\n",
    "        \n",
    "        self.board = np.zeros((height, width), dtype=int)\n",
    "        self.spawn_piece()\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset the environment to initial state\"\"\"\n",
    "        self.board = np.zeros((self.height, self.width), dtype=int)\n",
    "        self.score = 0\n",
    "        self.spawn_piece()\n",
    "        return self._get_state()\n",
    "    \n",
    "    def spawn_piece(self):\n",
    "        \"\"\"Spawn a new piece at the top of the board\"\"\"\n",
    "        self.current_piece = random.choice(list(SHAPES.keys()))\n",
    "        self.current_rotation = 0\n",
    "        self.current_pos = [0, self.width // 2 - 1]\n",
    "    \n",
    "    def _get_current_shape(self):\n",
    "        \"\"\"Get the current piece's shape based on rotation\"\"\"\n",
    "        return SHAPES[self.current_piece][self.current_rotation]\n",
    "    \n",
    "    def _is_valid_move(self, pos, shape):\n",
    "        \"\"\"Check if the given position is valid for the current piece\"\"\"\n",
    "        for block in shape:\n",
    "            y = pos[0] + block[0]\n",
    "            x = pos[1] + block[1]\n",
    "            if (x < 0 or x >= self.width or \n",
    "                y >= self.height or \n",
    "                (y >= 0 and self.board[y][x])):\n",
    "                return False\n",
    "        return True\n",
    "    \n",
    "    def _get_column_height(self, col):\n",
    "        \"\"\"Get the height of blocks in a given column\"\"\"\n",
    "        for row in range(self.height):\n",
    "            if self.board[row][col] == 1:\n",
    "                return self.height - row\n",
    "        return 0\n",
    "    \n",
    "    def _get_column_holes(self, col):\n",
    "        \"\"\"Count holes in a given column\"\"\"\n",
    "        holes = 0\n",
    "        block_found = False\n",
    "        for row in range(self.height):\n",
    "            if self.board[row][col] == 1:\n",
    "                block_found = True\n",
    "            elif block_found and self.board[row][col] == 0:\n",
    "                holes += 1\n",
    "        return holes\n",
    "    \n",
    "    def _get_max_height(self):\n",
    "        \"\"\"Get the maximum height of the stack\"\"\"\n",
    "        for y in range(self.height):\n",
    "            if 1 in self.board[y]:\n",
    "                return self.height - y\n",
    "        return 0\n",
    "    \n",
    "    def _count_holes(self):\n",
    "        \"\"\"Count total holes in the board\"\"\"\n",
    "        return sum(self._get_column_holes(col) for col in range(self.width))\n",
    "    \n",
    "    def _clear_lines(self):\n",
    "        \"\"\"Clear complete lines and return the number of lines cleared\"\"\"\n",
    "        lines_cleared = 0\n",
    "        y = self.height - 1\n",
    "        while y >= 0:\n",
    "            if np.all(self.board[y]):\n",
    "                self.board = np.vstack((np.zeros((1, self.width)), \n",
    "                                      self.board[:y], \n",
    "                                      self.board[y+1:]))\n",
    "                lines_cleared += 1\n",
    "            else:\n",
    "                y -= 1\n",
    "        return lines_cleared\n",
    "    \n",
    "    def _get_surface_roughness(self):\n",
    "        \"\"\"Calculate surface roughness\"\"\"\n",
    "        heights = [self._get_column_height(col) for col in range(self.width)]\n",
    "        return sum(abs(heights[i] - heights[i-1]) for i in range(1, len(heights)))\n",
    "    \n",
    "    def _detect_wells(self):\n",
    "        \"\"\"Detect and count wells suitable for I-pieces\"\"\"\n",
    "        wells = 0\n",
    "        heights = [self._get_column_height(col) for col in range(self.width)]\n",
    "        \n",
    "        for col in range(self.width):\n",
    "            if col == 0:\n",
    "                if heights[col] + 3 < heights[col + 1]:\n",
    "                    wells += 1\n",
    "            elif col == self.width - 1:\n",
    "                if heights[col] + 3 < heights[col - 1]:\n",
    "                    wells += 1\n",
    "            else:\n",
    "                if heights[col] + 3 < heights[col - 1] and heights[col] + 3 < heights[col + 1]:\n",
    "                    wells += 1\n",
    "        return wells\n",
    "    \n",
    "    def _get_state(self):\n",
    "        \"\"\"Get the current state representation\"\"\"\n",
    "        board_state = self.board.flatten()\n",
    "        piece_state = np.zeros(len(SHAPES))\n",
    "        piece_state[list(SHAPES.keys()).index(self.current_piece)] = 1\n",
    "        heights = np.array([self._get_column_height(col) for col in range(self.width)])\n",
    "        holes = np.array([self._get_column_holes(col) for col in range(self.width)])\n",
    "        bumpiness = np.diff(heights)\n",
    "        \n",
    "        return np.concatenate([\n",
    "            board_state,\n",
    "            piece_state,\n",
    "            heights / self.height,\n",
    "            holes / self.height,\n",
    "            bumpiness / self.height\n",
    "        ])\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"Execute one step in the environment\"\"\"\n",
    "        reward = 0\n",
    "        done = False\n",
    "        initial_height = self._get_max_height()\n",
    "        initial_holes = self._count_holes()\n",
    "        \n",
    "        # Handle actions\n",
    "        if action == 0:  # Move left\n",
    "            new_pos = [self.current_pos[0], self.current_pos[1] - 1]\n",
    "            if self._is_valid_move(new_pos, self._get_current_shape()):\n",
    "                self.current_pos = new_pos\n",
    "        elif action == 1:  # Move right\n",
    "            new_pos = [self.current_pos[0], self.current_pos[1] + 1]\n",
    "            if self._is_valid_move(new_pos, self._get_current_shape()):\n",
    "                self.current_pos = new_pos\n",
    "        elif action == 2:  # Rotate\n",
    "            new_rotation = (self.current_rotation + 1) % len(SHAPES[self.current_piece])\n",
    "            new_shape = SHAPES[self.current_piece][new_rotation]\n",
    "            if self._is_valid_move(self.current_pos, new_shape):\n",
    "                self.current_rotation = new_rotation\n",
    "        \n",
    "        # Try to move down\n",
    "        new_pos = [self.current_pos[0] + 1, self.current_pos[1]]\n",
    "        if self._is_valid_move(new_pos, self._get_current_shape()):\n",
    "            self.current_pos = new_pos\n",
    "        else:\n",
    "            # Place the piece\n",
    "            for block in self._get_current_shape():\n",
    "                y = self.current_pos[0] + block[0]\n",
    "                x = self.current_pos[1] + block[1]\n",
    "                if y >= 0:\n",
    "                    self.board[y][x] = 1\n",
    "            \n",
    "            # Calculate rewards\n",
    "            lines_cleared = self._clear_lines()\n",
    "            new_height = self._get_max_height()\n",
    "            new_holes = self._count_holes()\n",
    "            surface_roughness = self._get_surface_roughness()\n",
    "            \n",
    "            reward = self._calculate_reward(\n",
    "                lines_cleared,\n",
    "                new_height - initial_height,\n",
    "                new_holes - initial_holes,\n",
    "                surface_roughness\n",
    "            )\n",
    "            \n",
    "            # Spawn new piece\n",
    "            self.spawn_piece()\n",
    "            if not self._is_valid_move(self.current_pos, self._get_current_shape()):\n",
    "                done = True\n",
    "                reward -= self.reward_weights['game_over_penalty']\n",
    "        \n",
    "        return self._get_state(), reward, done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TetrisVisualizer:\n",
    "    def __init__(self, env, cell_size=30):\n",
    "        \"\"\"Initialize the Tetris visualizer\"\"\"\n",
    "        pygame.init()\n",
    "        self.env = env\n",
    "        self.cell_size = cell_size\n",
    "        self.width = env.width * cell_size\n",
    "        self.height = env.height * cell_size\n",
    "        \n",
    "        # Add padding for score display\n",
    "        self.padding = 100\n",
    "        self.screen = pygame.display.set_mode((self.width + self.padding, self.height))\n",
    "        pygame.display.set_caption('Tetris AI')\n",
    "        \n",
    "        # Initialize fonts\n",
    "        self.font = pygame.font.Font(None, 36)\n",
    "        \n",
    "        # Add gradient effects\n",
    "        self.gradient_colors = self._create_gradient()\n",
    "    \n",
    "    def _create_gradient(self):\n",
    "        \"\"\"Create a subtle gradient effect for blocks\"\"\"\n",
    "        gradient = {}\n",
    "        for piece, color in self.env.colors.items():\n",
    "            if isinstance(color, tuple) and piece != 'grid':\n",
    "                # Handle both RGB and RGBA colors\n",
    "                if len(color) == 4:  # RGBA\n",
    "                    r, g, b, a = color\n",
    "                else:  # RGB\n",
    "                    r, g, b = color\n",
    "                gradient[piece] = {\n",
    "                    'main': (r, g, b),\n",
    "                    'light': (min(r + 30, 255), min(g + 30, 255), min(b + 30, 255)),\n",
    "                    'dark': (max(r - 30, 0), max(g - 30, 0), max(b - 30, 0))\n",
    "                }\n",
    "        return gradient\n",
    "    \n",
    "    def _draw_block(self, x, y, color_key):\n",
    "        \"\"\"Draw a single block with gradient effect\"\"\"\n",
    "        if color_key in self.gradient_colors:\n",
    "            colors = self.gradient_colors[color_key]\n",
    "            # Main block\n",
    "            pygame.draw.rect(self.screen, colors['main'],\n",
    "                           [x, y, self.cell_size, self.cell_size])\n",
    "            # Light edge\n",
    "            pygame.draw.line(self.screen, colors['light'],\n",
    "                           (x, y), (x + self.cell_size, y), 2)\n",
    "            pygame.draw.line(self.screen, colors['light'],\n",
    "                           (x, y), (x, y + self.cell_size), 2)\n",
    "            # Dark edge\n",
    "            pygame.draw.line(self.screen, colors['dark'],\n",
    "                           (x + self.cell_size, y), (x + self.cell_size, y + self.cell_size), 2)\n",
    "            pygame.draw.line(self.screen, colors['dark'],\n",
    "                           (x, y + self.cell_size), (x + self.cell_size, y + self.cell_size), 2)\n",
    "        else:\n",
    "            # Fallback for pieces without gradient colors\n",
    "            pygame.draw.rect(self.screen, self.env.colors.get(color_key, (128, 128, 128)),\n",
    "                           [x, y, self.cell_size, self.cell_size])\n",
    "    \n",
    "    def _get_ghost_position(self):\n",
    "        \"\"\"Calculate the position where the current piece would land\"\"\"\n",
    "        if not self.env.current_piece:\n",
    "            return 0\n",
    "        \n",
    "        ghost_y = self.env.current_pos[0]\n",
    "        while self.env._is_valid_move([ghost_y + 1, self.env.current_pos[1]], \n",
    "                                    self.env._get_current_shape()):\n",
    "            ghost_y += 1\n",
    "        return ghost_y\n",
    "    \n",
    "    def draw_board(self):\n",
    "        \"\"\"Draw the complete game board with all pieces and UI elements\"\"\"\n",
    "        self.screen.fill(self.env.colors['background'])\n",
    "        \n",
    "        # Draw the grid\n",
    "        for y in range(self.env.height):\n",
    "            fade = 1 - (y / self.env.height * 0.5)  # Creates a subtle fade effect\n",
    "            grid_color = tuple(int(c * fade) for c in self.env.colors['grid'])\n",
    "            for x in range(self.env.width):\n",
    "                pygame.draw.rect(self.screen, grid_color,\n",
    "                               [x * self.cell_size, y * self.cell_size,\n",
    "                                self.cell_size, self.cell_size], 1)\n",
    "        \n",
    "        # Draw ghost piece if there is a current piece\n",
    "        if self.env.current_piece:\n",
    "            ghost_y = self._get_ghost_position()\n",
    "            ghost_color = self.env.colors['ghost']\n",
    "            if len(ghost_color) == 3:  # Convert RGB to RGBA\n",
    "                ghost_color = (*ghost_color, 128)\n",
    "            ghost_surface = pygame.Surface((self.cell_size, self.cell_size), pygame.SRCALPHA)\n",
    "            pygame.draw.rect(ghost_surface, ghost_color, ghost_surface.get_rect())\n",
    "            \n",
    "            for block in self.env._get_current_shape():\n",
    "                x = (self.env.current_pos[1] + block[1]) * self.cell_size\n",
    "                y = (ghost_y + block[0]) * self.cell_size\n",
    "                if ghost_y + block[0] >= 0:\n",
    "                    self.screen.blit(ghost_surface, (x, y))\n",
    "        \n",
    "        # Draw placed blocks\n",
    "        for y in range(self.env.height):\n",
    "            for x in range(self.env.width):\n",
    "                if self.env.board[y][x]:\n",
    "                    self._draw_block(x * self.cell_size, y * self.cell_size, \n",
    "                                   self.env.current_piece)\n",
    "        \n",
    "        # Draw current piece\n",
    "        if self.env.current_piece:\n",
    "            for block in self.env._get_current_shape():\n",
    "                x = (self.env.current_pos[1] + block[1]) * self.cell_size\n",
    "                y = (self.env.current_pos[0] + block[0]) * self.cell_size\n",
    "                if self.env.current_pos[0] + block[0] >= 0:\n",
    "                    self._draw_block(x, y, self.env.current_piece)\n",
    "        \n",
    "        # Draw score and other stats\n",
    "        self._draw_stats()\n",
    "        \n",
    "        pygame.display.flip()\n",
    "    \n",
    "    def _draw_stats(self):\n",
    "        \"\"\"Draw score and other statistics\"\"\"\n",
    "        score_text = self.font.render(f'Score: {self.env.score}', True, (255, 255, 255))\n",
    "        self.screen.blit(score_text, (self.width + 10, 20))\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"Clean up and close the visualization\"\"\"\n",
    "        pygame.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 256)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.fc3 = nn.Linear(256, 128)\n",
    "        self.fc4 = nn.Linear(128, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.dropout(self.relu(self.fc1(x)))\n",
    "        x = self.dropout(self.relu(self.fc2(x)))\n",
    "        x = self.relu(self.fc3(x))\n",
    "        return self.fc4(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "    \n",
    "    def push(self, *args):\n",
    "        self.memory.append(Transition(*args))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TetrisAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        \n",
    "        self.policy_net = DQN(state_size, action_size).to(self.device)\n",
    "        self.target_net = DQN(state_size, action_size).to(self.device)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        \n",
    "        self.batch_size = 256\n",
    "        self.gamma = 0.99\n",
    "        self.eps_start = 1.0\n",
    "        self.eps_end = 0.01\n",
    "        self.eps_decay = 2000\n",
    "        self.target_update = 5\n",
    "        self.learning_rate = 0.0001\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=self.learning_rate)\n",
    "        self.memory = ReplayMemory(10000)\n",
    "        self.steps_done = 0\n",
    "        \n",
    "        # Training statistics\n",
    "        self.episodes_completed = 0\n",
    "        self.best_score = float('-inf')\n",
    "        self.recent_scores = deque(maxlen=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_state(self, filename):\n",
    "        try:\n",
    "            state = {\n",
    "                'policy_net_state': self.policy_net.state_dict(),\n",
    "                'target_net_state': self.target_net.state_dict(),\n",
    "                'optimizer_state': self.optimizer.state_dict(),\n",
    "                'steps_done': self.steps_done,\n",
    "                'memory': list(self.memory.memory),\n",
    "                'epsilon': self.eps_start,\n",
    "                'training_stats': {\n",
    "                    'episodes_completed': self.episodes_completed,\n",
    "                    'best_score': self.best_score,\n",
    "                    'recent_scores': list(self.recent_scores)\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            directory = 'saved_models'\n",
    "            os.makedirs(directory, exist_ok=True)\n",
    "            \n",
    "            filepath = os.path.join(directory, filename)\n",
    "            torch.save(state, filepath)\n",
    "            print(f\"Model saved to {filepath}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving model: {e}\")\n",
    "    \n",
    "def load_state(self, filename):\n",
    "        try:\n",
    "            filepath = os.path.join('saved_models', filename)\n",
    "            if not os.path.exists(filepath):\n",
    "                print(f\"No saved model found at {filepath}\")\n",
    "                return False\n",
    "            \n",
    "            state = torch.load(filepath, map_location=self.device)\n",
    "            \n",
    "            self.policy_net.load_state_dict(state['policy_net_state'])\n",
    "            self.target_net.load_state_dict(state['target_net_state'])\n",
    "            self.optimizer.load_state_dict(state['optimizer_state'])\n",
    "            self.steps_done = state['steps_done']\n",
    "            \n",
    "            # Restore training statistics\n",
    "            training_stats = state.get('training_stats', {})\n",
    "            self.episodes_completed = training_stats.get('episodes_completed', 0)\n",
    "            self.best_score = training_stats.get('best_score', float('-inf'))\n",
    "            self.recent_scores = deque(training_stats.get('recent_scores', []), maxlen=100)\n",
    "            \n",
    "            # Restore replay memory\n",
    "            self.memory = ReplayMemory(self.memory.capacity)\n",
    "            self.memory.memory = deque(state['memory'], maxlen=self.memory.capacity)\n",
    "            \n",
    "            print(f\"Model loaded from {filepath}\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model: {e}\")\n",
    "            return False\n",
    "            \n",
    "def update_epsilon(self):\n",
    "    \"\"\"Update exploration rate\"\"\"\n",
    "    self.eps_start = max(self.eps_end, self.eps_start * 0.995)  # Decay epsilon\n",
    "    \n",
    "def get_state_dict(self):\n",
    "    \"\"\"Get a dictionary of current agent state\"\"\"\n",
    "    return {\n",
    "        'steps': self.steps_done,\n",
    "        'episodes': self.episodes_completed,\n",
    "        'best_score': self.best_score,\n",
    "        'recent_avg': np.mean(self.recent_scores) if self.recent_scores else 0,\n",
    "        'epsilon': self.eps_start\n",
    "    }\n",
    "    \n",
    "def add_experience(self, state, action, next_state, reward, done):\n",
    "    \"\"\"Add an experience to memory and handle end-of-episode tasks\"\"\"\n",
    "    # Store transition in memory\n",
    "    self.memory.push(state, action, next_state, reward)\n",
    "    \n",
    "    if done:\n",
    "        self.episodes_completed += 1\n",
    "        self.update_epsilon()\n",
    "        \n",
    "def train_step(self, state, action, next_state, reward, done):\n",
    "    \"\"\"Perform a single training step\"\"\"\n",
    "    # Add experience to memory\n",
    "    self.add_experience(state, action, next_state, reward, done)\n",
    "    \n",
    "    # Optimize model\n",
    "    self.optimize_model()\n",
    "    \n",
    "    # Update target network if needed\n",
    "    if self.steps_done % (self.target_update * self.batch_size) == 0:\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        \n",
    "    return self.get_state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(self, state):\n",
    "        sample = random.random()\n",
    "        eps_threshold = self.eps_end + (self.eps_start - self.eps_end) * \\\n",
    "                       np.exp(-1. * self.steps_done / self.eps_decay)\n",
    "        self.steps_done += 1\n",
    "        \n",
    "        if sample > eps_threshold:\n",
    "            with torch.no_grad():\n",
    "                state = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "                return self.policy_net(state).max(1)[1].view(1, 1)\n",
    "        else:\n",
    "            return torch.tensor([[random.randrange(self.action_size)]], \n",
    "                              device=self.device, dtype=torch.long)\n",
    "    \n",
    "def optimize_model(self):\n",
    "    if len(self.memory) < self.batch_size:\n",
    "        return\n",
    "    \n",
    "    transitions = self.memory.sample(self.batch_size)\n",
    "    batch = Transition(*zip(*transitions))\n",
    "    \n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), \n",
    "                                device=self.device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.FloatTensor([s for s in batch.next_state\n",
    "                                             if s is not None]).to(self.device)\n",
    "    \n",
    "    state_batch = torch.FloatTensor(batch.state).to(self.device)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.FloatTensor(batch.reward).to(self.device)\n",
    "    \n",
    "    state_action_values = self.policy_net(state_batch).gather(1, action_batch)\n",
    "    \n",
    "    next_state_values = torch.zeros(self.batch_size, device=self.device)\n",
    "    next_state_values[non_final_mask] = self.target_net(non_final_next_states).max(1)[0].detach()\n",
    "    expected_state_action_values = (next_state_values * self.gamma) + reward_batch\n",
    "    \n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "    \n",
    "    self.optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in self.policy_net.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    self.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_visualization(num_episodes=1000, load_file=None, save_file='tetris_model.pth'):\n",
    "    env = TetrisEnv()\n",
    "    visualizer = TetrisVisualizer(env)\n",
    "    state_size = 200 + len(SHAPES) + env.width * 2 + (env.width - 1)  # Updated state size\n",
    "    action_size = 4\n",
    "    \n",
    "    agent = TetrisAgent(state_size, action_size)\n",
    "    \n",
    "    # Load previous state if specified\n",
    "    if load_file:\n",
    "        agent.load_state(load_file)\n",
    "    \n",
    "    scores = []\n",
    "    best_score = float('-inf')\n",
    "    \n",
    "    try:\n",
    "        for episode in range(num_episodes):\n",
    "            state = env.reset()\n",
    "            episode_score = 0\n",
    "            done = False\n",
    "            \n",
    "            # Play one episode\n",
    "            while not done:\n",
    "                # Select and perform an action\n",
    "                action = agent.select_action(state)\n",
    "                next_state, reward, done = env.step(action.item())\n",
    "                episode_score += reward\n",
    "                \n",
    "                # Store the transition in memory\n",
    "                agent.memory.push(state, action, next_state, reward)\n",
    "                \n",
    "                # Move to the next state\n",
    "                state = next_state\n",
    "                \n",
    "                # Perform optimization step\n",
    "                agent.optimize_model()\n",
    "                \n",
    "                # Visualize every 10th episode\n",
    "                if episode % 10 == 0:\n",
    "                    visualizer.draw_board()\n",
    "                    pygame.event.pump()  # Handle pygame events\n",
    "                    time.sleep(0.05)  # Add small delay for visualization\n",
    "            \n",
    "            # Update training statistics\n",
    "            scores.append(episode_score)\n",
    "            agent.recent_scores.append(episode_score)\n",
    "            if episode_score > best_score:\n",
    "                best_score = episode_score\n",
    "                agent.best_score = best_score\n",
    "            \n",
    "            # Update the target network every few episodes\n",
    "            if episode % agent.target_update == 0:\n",
    "                agent.target_net.load_state_dict(agent.policy_net.state_dict())\n",
    "            \n",
    "            # Save the model periodically\n",
    "            if episode % 100 == 0:\n",
    "                agent.save_state(save_file)\n",
    "            \n",
    "            # Print progress every 10 episodes\n",
    "            if episode % 10 == 0:\n",
    "                avg_score = np.mean(agent.recent_scores)\n",
    "                print(f'Episode {episode}/{num_episodes}, Score: {episode_score:.2f}, Avg Score: {avg_score:.2f}, Best: {best_score:.2f}')\n",
    "                \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nTraining interrupted by user\")\n",
    "        agent.save_state(save_file)  # Save on interrupt\n",
    "    finally:\n",
    "        visualizer.close()\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_agent(agent, num_episodes=10, visualize=True):\n",
    "    env = TetrisEnv()\n",
    "    visualizer = TetrisVisualizer(env) if visualize else None\n",
    "    scores = []\n",
    "    \n",
    "    try:\n",
    "        for episode in range(num_episodes):\n",
    "            state = env.reset()\n",
    "            episode_score = 0\n",
    "            done = False\n",
    "            \n",
    "            while not done:\n",
    "                # Select action without exploration\n",
    "                with torch.no_grad():\n",
    "                    state_tensor = torch.FloatTensor(state).unsqueeze(0).to(agent.device)\n",
    "                    action = agent.policy_net(state_tensor).max(1)[1].view(1, 1)\n",
    "                \n",
    "                # Perform action\n",
    "                next_state, reward, done = env.step(action.item())\n",
    "                episode_score += reward\n",
    "                state = next_state\n",
    "                \n",
    "                # Visualize if requested\n",
    "                if visualize:\n",
    "                    visualizer.draw_board()\n",
    "                    pygame.event.pump()\n",
    "                    time.sleep(0.1)\n",
    "            \n",
    "            scores.append(episode_score)\n",
    "            print(f'Evaluation Episode {episode + 1}/{num_episodes}, Score: {episode_score:.2f}')\n",
    "    \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nEvaluation interrupted by user\")\n",
    "    finally:\n",
    "        if visualize:\n",
    "            visualizer.close()\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_tetris_training(training_episodes=1000, eval_episodes=5, save_interval=100, \n",
    "                       load_file=None, save_file='tetris_model.pth'):\n",
    "    # Train the agent\n",
    "    print(\"Starting training...\")\n",
    "    training_scores = train_with_visualization(\n",
    "        num_episodes=training_episodes,\n",
    "        load_file=load_file,\n",
    "        save_file=save_file\n",
    "    )\n",
    "    \n",
    "    # Plot training progress\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(training_scores)\n",
    "    plt.title('Training Progress')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Score')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot moving average\n",
    "    window_size = 50\n",
    "    moving_avg = np.convolve(training_scores, np.ones(window_size)/window_size, mode='valid')\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(moving_avg)\n",
    "    plt.title(f'Moving Average Score (Window Size: {window_size})')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Average Score')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    # Evaluate the trained agent\n",
    "    print(\"\\nEvaluating trained agent...\")\n",
    "    env = TetrisEnv()\n",
    "    state_size = 200 + len(SHAPES) + env.width * 2 + (env.width - 1)\n",
    "    action_size = 4\n",
    "    agent = TetrisAgent(state_size, action_size)\n",
    "    \n",
    "    # Load the final saved model for evaluation\n",
    "    agent.load_state(save_file)\n",
    "    \n",
    "    eval_scores = evaluate_agent(agent, num_episodes=eval_episodes)\n",
    "    \n",
    "    print(f\"\\nEvaluation Results:\")\n",
    "    print(f\"Average Score: {np.mean(eval_scores):.2f}\")\n",
    "    print(f\"Best Score: {np.max(eval_scores):.2f}\")\n",
    "    print(f\"Worst Score: {np.min(eval_scores):.2f}\")\n",
    "    \n",
    "    return training_scores, eval_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'TetrisAgent' object has no attribute 'load_state'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# To start fresh training:\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# training_scores, eval_scores = run_tetris_training(training_episodes=1000)\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# To continue training from a saved model:\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m training_scores, eval_scores \u001b[38;5;241m=\u001b[39m \u001b[43mrun_tetris_training\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtetris_model.pth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[0;32m      8\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[14], line 5\u001b[0m, in \u001b[0;36mrun_tetris_training\u001b[1;34m(training_episodes, eval_episodes, save_interval, load_file, save_file)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_tetris_training\u001b[39m(training_episodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m, eval_episodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, save_interval\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, \n\u001b[0;32m      2\u001b[0m                        load_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, save_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtetris_model.pth\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;66;03m# Train the agent\u001b[39;00m\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting training...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m     training_scores \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_with_visualization\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_episodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43mload_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mload_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_file\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;66;03m# Plot training progress\u001b[39;00m\n\u001b[0;32m     12\u001b[0m     plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n",
      "Cell \u001b[1;32mIn[12], line 11\u001b[0m, in \u001b[0;36mtrain_with_visualization\u001b[1;34m(num_episodes, load_file, save_file)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Load previous state if specified\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m load_file:\n\u001b[1;32m---> 11\u001b[0m     \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state\u001b[49m(load_file)\n\u001b[0;32m     13\u001b[0m scores \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     14\u001b[0m best_score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-inf\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'TetrisAgent' object has no attribute 'load_state'"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# To start fresh training:\n",
    "# training_scores, eval_scores = run_tetris_training(training_episodes=1000)\n",
    "\n",
    "# To continue training from a saved model:\n",
    "training_scores, eval_scores = run_tetris_training(\n",
    "    training_episodes=1000,\n",
    "    load_file='tetris_model.pth'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
